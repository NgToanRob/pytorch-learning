{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `__len__`: trả về số sample trong dataset, tương ứng len(dataset).\n",
    "- `__getitem__`:  lấy ra sample thứ i trong dataset, tương ứng dataset[i].\n",
    "\n",
    "![](https://i0.wp.com/nttuan8.com/wp-content/uploads/2021/03/image-39.png?resize=1024%2C420&ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ như khi load dataset cifar10 từ torchvision.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "data_path = 'data'\n",
    "\n",
    "# load dataset cifar10\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "50000\n",
      "(<PIL.Image.Image image mode=RGB size=32x32 at 0x7F960816C790>, 6)\n"
     ]
    }
   ],
   "source": [
    "# Check instance is Pytorch Dataset\n",
    "print(isinstance(cifar10, torch.utils.data.Dataset))\n",
    "# Get len\n",
    "print(cifar10.__len__())\n",
    "#Get an item\n",
    "print(cifar10[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '6')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img, label = cifar10[0]\n",
    "plt.imshow(img)\n",
    "plt.title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "Dữ liệu mình lấy được ở trên thì ảnh ở dạng PIL image, mình cần convert về dạng Torch tensor để cho Pytorch xử lý và tính toán. Module torchvision.transforms hỗ trợ các phép chuyển đổi trên ảnh như: chuyển sang tensor, normalization, augmentation,…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "to_tensor = transforms.ToTensor()\n",
    "# function convert PIL image to tensor\n",
    "print(type(img))\n",
    "img_tensor = to_tensor(img)\n",
    "print(type(img_tensor))\n",
    "print(img_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mình có thể transform trong lớp dataset khi load dữ liệu cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "tensor_cifar10 = datasets.CIFAR10(\n",
    "    data_path,\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "img_t, label = tensor_cifar10[99]\n",
    "print(type(img_t))\n",
    "print(type(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ảnh ở dạng PIL image thì các pixel có giá trị từ 0-255, tuy nhiên khi chuyển về dạng tensor (ToTensor) thì dữ liệu pixel được scale về khoảng 0.0-1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(img_t.min(), img_t.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "Việc normalization dữ liệu giúp các pixel có cùng scale cũng như distribution, do đó mình có thể dùng thuật toán gradient descent cho các tham số với cùng một learning rate. Thông thường mình sẽ normalize để mỗi channel về standard normal distribution (N(0, 1), normal distribution với 0 mean và 1 standard deviation)\n",
    "\n",
    "$\\displaystyle v[c] = \\frac{v[c] - mean[c]}{std[c]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs.view(3, -1).mean(dim=1) # tensor([0.4915, 0.4823, 0.4468])\n",
    "# imgs.view(3, -1).std(dim=1) # tensor([0.2470, 0.2435, 0.2616])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compose\n",
    "Ngoài ra Pytorch transforms còn hỗ trợ rất nhiều phép biển đổi khác như: RandomCrop, RandomRotation, RandomAffine (để shear), RandomHorizontalFlip,… Chi tiết mọi người xem ở [đây](https://pytorch.org/vision/stable/transforms.html).\n",
    "\n",
    "Để thực hiện nhiều phép biến đổi trên dữ liệu đầu vào, transforms hỗ trợ hàm compose để gộp các transforms lại."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    " (0.2470, 0.2435, 0.2616))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_cifar10_train = datasets.CIFAR10(\n",
    "    data_path,\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transform\n",
    ")\n",
    "tensor_cifar10_val = datasets.CIFAR10(\n",
    "    data_path,\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như vậy là mình đã có Dataset và thực hiện transform các ảnh đưa về dạng tensor cũng như normalize các ảnh. Giờ mình cần lấy các ảnh cho quá trình traning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "Khi cho dữ liệu vào model để học thì thông thường sẽ cho dữ liệu theo từng `batch` một, `DataLoader` sẽ giúp chúng ta lấy dữ liệu theo từng `batch`, `shuffle` dữ liệu cũng như load dữ liệu song song với nhiều `multiprocessing workers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    tensor_cifar10_train,\n",
    "    batch_size=36,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    tensor_cifar10_val,\n",
    "    batch_size=36,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Build a simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        # bài toán phân loại 10 lớp nên output ra 10 nodes\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        # flatten về dạng vector để cho vào neural network\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu ý: Hàm CrossEntropyLoss của Pytorch đã bao gồm cả `activation softmax` ở lớp output và `categorial crossentropy loss` thế nên khi dựng `model` không cần dùng `activation softmax` ở `output layer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_loop(n_epochs, optimizer, model, loss_fn, train_loader, val_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in tqdm(train_loader):\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        correct = 0\n",
    "        # Compute acc in valid dataset\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(val_loader):\n",
    "                images, labels = data\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                c = (predicted == labels).squeeze()\n",
    "                correct += c.sum()\n",
    "        if epoch == 1 or epoch%1 ==0:\n",
    "            print('Epoch {}, Training loss {}, Valid accuracy {}'.format(\n",
    "                epoch,\n",
    "                loss_train/len(train_loader),\n",
    "                correct/ len(val_loader)\n",
    "            ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 599/1389 [00:20<00:27, 28.60it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [42], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m3e-2\u001b[39m)\n\u001b[1;32m      3\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 5\u001b[0m train_loop(\n\u001b[1;32m      6\u001b[0m     n_epochs\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m,\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[1;32m      8\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m      9\u001b[0m     loss_fn\u001b[39m=\u001b[39mloss_fn,\n\u001b[1;32m     10\u001b[0m     train_loader\u001b[39m=\u001b[39mtrain_loader,\n\u001b[1;32m     11\u001b[0m     val_loader\u001b[39m=\u001b[39mval_loader\n\u001b[1;32m     12\u001b[0m )\n",
      "Cell \u001b[0;32mIn [41], line 5\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(n_epochs, optimizer, model, loss_fn, train_loader, val_loader)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m      4\u001b[0m     loss_train \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m     \u001b[39mfor\u001b[39;00m imgs, labels \u001b[39min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m      6\u001b[0m         outputs \u001b[39m=\u001b[39m model(imgs)\n\u001b[1;32m      7\u001b[0m         loss \u001b[39m=\u001b[39m loss_fn(outputs, labels)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    128\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:134\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_tensor\u001b[39m(pic) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    123\u001b[0m     \u001b[39m\"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m    This function does not support torchscript.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[1;32m    135\u001b[0m         _log_api_usage_once(to_tensor)\n\u001b[1;32m    136\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (F_pil\u001b[39m.\u001b[39m_is_pil_image(pic) \u001b[39mor\u001b[39;00m _is_numpy(pic)):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_jit_internal.py:1082\u001b[0m, in \u001b[0;36mis_scripting\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39m7\u001b[39m):\n\u001b[1;32m   1079\u001b[0m     \u001b[39mglobals\u001b[39m()[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBroadcastingList\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m BroadcastingList1\n\u001b[0;32m-> 1082\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_scripting\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m   1083\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m \u001b[39m    Function that returns True when in compilation and False otherwise. This\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m \u001b[39m    is useful especially with the @unused decorator to leave code in your\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[39m              return unsupported_linear_op(x)\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=3e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loop(\n",
    "    n_epochs=30,\n",
    "    optimizer=optimizer,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
