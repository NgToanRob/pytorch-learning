{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 3: Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network\n",
    "Dữ liệu từ input layer sẽ được tính feedforward qua các hidden layer để ra được output layer.\n",
    "![](https://i1.wp.com/nttuan8.com/wp-content/uploads/2019/03/fw.png?fit=1024%2C98&ssl=1)\n",
    "\n",
    "Sau khi có giá trị dữ đoán $\\hat{y}$ thì mình sẽ tạo loss function và dùng thuật toán backpropagation để tính đạo hàm của loss với các tham số W, b, rồi dùng thuật toán gradient descent để cập nhật hệ số và tối ưu loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Linear(số_node_input, số node_output) trong layer đó\n",
    "linear_model = nn.Linear(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.9829]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.5027], requires_grad=True)]\n",
      "Total parameters: 2\n"
     ]
    }
   ],
   "source": [
    "print(list(linear_model.parameters()))\n",
    "print(f\"Total parameters: {len(list(linear_model.parameters()))}\")\n",
    "\n",
    "# [Parameter containing: tensor([[14.9464]], requires_grad=True), Parameter containing: tensor([1.1261], requires_grad=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong mỗi layer sẽ gồm 2 Parameters là `W` và `b` tương ứng. Mình thấy là thuộc tính `requires_grad` của các Parameter đều là `True` để có thể tính `backward` loss và dùng gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong bài trước mình phải tự tính `L.backward()` rồi thực hiện `gradient descent` bằng tay, tuy nhiên là Pytorch hỗ trợ các `optimizer` trong `nn.optim` để giúp mọi người thực hiện `gradient descent` luôn. Mình cần truyền cho `optimizer` biết là: mình muốn thực hiện `gradient descent` với những tham số nào (thường là tất cả các tham số trong `model`) cũng như `learning rate` là bao nhiêu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(linear_model.parameters(), lr=0.00004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop\n",
    "```python\n",
    "for epoch in tqdm(range(1, n_epochs+1)):\n",
    "    y_hat = model(X)\n",
    "    loss = loss_fn(y_hat, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mình tính giá trị dự đoán, sau đó tính `loss`, rồi gọi `loss.backward()` để tính đạo hàm ngược lại cho các `W`, `b`. *Tuy nhiên* vì mỗi khi gọi `loss.backward()` thì đạo hàm ở các tham số sẽ cộng dồn lại, nên mình cần gọi zero_grad để gán đạo hàm ở các tham số bằng 0 trước khi gọi hàm backward(). Cuối cùng gọi `optimizer.step()` để thực hiện `update gradient descent` trên các tham số trong `optimizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi mình cho dữ liệu vào `model` để học, thì thay vì cho học từng dữ liệu một, mình sẽ cho học nhiều điểm dữ liệu một lúc theo `batch`, dữ liệu mình truyền vào sẽ có kích thước `(batch_size, 1)` hay trong trường hợp tổng quát với NN là `(batch_size, num_features)`, viết tắt `(N*d)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "x = torch.tensor(data[:,0], dtype=torch.float32) # x.shape (30)\n",
    "# Thêm chiều vào vị trí tương ứng, ví dụ (30) -> (30, 1).\n",
    "x = x.unsqueeze(1) # x.shape (30, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuối cùng xong mình sẽ tìm được model, các điểm xanh dương là dữ liệu của mình. Còn đường đỏ là model dự đoán.\n",
    "![](https://i0.wp.com/nttuan8.com/wp-content/uploads/2021/03/image-38.png?w=604&ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mình sẽ xây dựng mạng CNN cho bài toán phân loại ảnh MNIST.\n",
    "\n",
    "Dữ liệu ảnh trong dataset MNIST là ảnh xám và có kích thước 28*28. Bài toán input là 1 ảnh, output xem ảnh đấy là số mấy 0->9.\n",
    "![](https://i2.wp.com/nttuan8.com/wp-content/uploads/2019/04/model.png?fit=1024%2C386&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(CNN, self).__init__()\n",
    "        # input 1 channel, output 32 channel, kernel size 3*3\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(14*14*32, 128)  # 14*14 from image dimension\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "        \n",
    "    # def forward(self, x):\n",
    "    #     # Max pooling 2*2\n",
    "    #     x = F.relu(self.conv1(x))\n",
    "\n",
    "    #     x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "    #     # Flatten về dạng vector để input vào mạng NN\n",
    "    #     x = torch.flatten(x)\n",
    "    #     x = F.relu(self.fc1(x))\n",
    "    #     x = self.fc2(x)\n",
    "    #     return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=6272, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = CNN()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total weights: 813802\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(\"total weights: %d\" %(sum(p.numel() for p in params))) # 8, do có 4 layers, mỗi layer có 2 Parameter là W và b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0468, -0.0544,  0.0390,  0.0508, -0.0747,  0.0463, -0.1495, -0.0909,\n",
      "        -0.1038,  0.0506], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# shape: batch_size * depth * height * weight (N*C*H*W)\n",
    "input = torch.randn(1, 1, 28, 28)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hooks functions\n",
    "- Forward hook\n",
    "- Backward hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InsideConv2dforward\n",
      "\n",
      "input:  <class 'tuple'> , len:  1\n",
      "input[0]:  <class 'torch.Tensor'> , shape:  torch.Size([1, 1, 28, 28])\n",
      "output:  <class 'torch.Tensor'> , len:  1 torch.Size([1, 32, 28, 28])\n",
      "InsideConv2dforward\n",
      "\n",
      "input:  <class 'tuple'> , len:  1\n",
      "input[0]:  <class 'torch.Tensor'> , shape:  torch.Size([1, 32, 28, 28])\n",
      "output:  <class 'torch.Tensor'> , len:  1 torch.Size([1, 32, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "net = CNN()\n",
    "def print_info(self, input, output):\n",
    "    # input is a tuple of inputs\n",
    "    # Output is a tensor, output value is in output.data\n",
    "    print('Inside' + self.__class__.__name__ + 'forward')\n",
    "    \n",
    "    print('')\n",
    "    print('input: ', type(input), ', len: ', len(input))\n",
    "    print('input[0]: ', type(input[0]), ', shape: ', input[0].shape)\n",
    "    print('output: ', type(output), ', len: ', len(output), output.data.shape)\n",
    "\n",
    "# Register a function to forward hook\n",
    "net.conv1.register_forward_hook(print_info)\n",
    "net.conv2.register_forward_hook(print_info)\n",
    "\n",
    "# Forward hook function will be called when model computing forward through conv1\n",
    "out = net(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tương tự, mình cũng có thể register 1 hàm thành backward hook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside Conv2d backward\n",
      "grad_input:  <class 'tuple'> , len:  3\n",
      "grad_output:  <class 'tuple'> , len:  1\n",
      "grad_output[0]:  <class 'torch.Tensor'> , size:  torch.Size([1, 32, 28, 28])\n",
      "\n",
      "Inside Conv2d backward\n",
      "grad_input:  <class 'tuple'> , len:  3\n",
      "grad_output:  <class 'tuple'> , len:  1\n",
      "grad_output[0]:  <class 'torch.Tensor'> , size:  torch.Size([1, 32, 28, 28])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toan/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "net = CNN()\n",
    "\n",
    "def print_backward_info(self, grad_input, grad_output):\n",
    "    # Grad_input and grad_output are tuples\n",
    "    print('Inside ' + self.__class__.__name__ + ' backward')\n",
    "    \n",
    "    print('grad_input: ', type(grad_input), ', len: ', len(grad_input))\n",
    "    print('grad_output: ', type(grad_output), ', len: ', len(grad_output))\n",
    "    print('grad_output[0]: ', type(grad_output[0]), ', size: ', grad_output[0].shape)\n",
    "    print('')\n",
    "\n",
    "# Register a function to backward hook\n",
    "net.conv1.register_backward_hook(print_backward_info)\n",
    "net.conv2.register_backward_hook(print_backward_info)\n",
    "\n",
    "out = net(input)\n",
    "target = torch.rand(10)\n",
    "err = loss_fn(out, target)\n",
    "\n",
    "\n",
    "# Backward hook will be called when loss backward through layer conv1 and conv2\n",
    "err.backward()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model __call__ vs forward\n",
    "Về cơ bản thì khi mọi người build xong model thì kết quả của việc mọi người dùng __call__ (net(input)) hay dùng forward là như nhau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 28, 28)\n",
    "out_call = net(input)\n",
    "out_forward = net.forward(input)\n",
    "\n",
    "out_call == out_forward # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuy nhiên __call__ ngoài việc thực hiện forward sẽ gọi các hook nữa, thế nên khi dùng mọi người nên dùng __call__ và tránh dùng forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __call__(self, *input, **kwargs):\n",
    "  for hook in self._forward_pre_hooks.values():\n",
    "    hook(self, input)\n",
    "  \n",
    "  result = self.forward(*input, **kwargs)\n",
    "  \n",
    "  for hook in self._forward_hooks.values():\n",
    "    hook(self, input, result)\n",
    "    # TODO\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.relu vs F.relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mọi người để ý ở trên, để áp dụng ReLU activation mình dùng `F.relu(...)`. Tuy nhiên trong module torch.nn cũng có `torch.nn.ReLU(...)`.\n",
    "\n",
    "Về cơ bản thì 2 hàm thực hiện chức năng giống nhau, áp dụng ReLU activation function. Tuy nhiên điểm khác nhau lớn nhất là: F.relu như một hàm tính ReLU bình thường, tuy nhiên nn.ReLU tạo ra 1 nn.Module giống như các layer Linear, Conv2d. Do đó nn.ReLU có thể thêm vào nn.Sequential, cũng như register các hàm hook.\n",
    "\n",
    "Những hàm để xây dựng model như Linear, Conv2d, ReLU, max_pool2d đều có cả 2 dạng nn và F. Nó phụ thuộc vào phong cách code và xây dựng model của mỗi người. Mình thì thường dùng nn với những hàm có tham số, còn không có tham số như ReLU, max_pool2d thì dùng functional (F)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
